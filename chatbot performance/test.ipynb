{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Do**\n",
    "- Add parts to Testing PDF Loaders<br>\n",
    " <small>(PDF Loaders + llm model version 을 복합적으로 비교할 수 있도록 테스트 만들기. 현재는 llm model 성능만 비교하도록 구성하였음)</small><br>\n",
    "\n",
    "- Make human-made questions for testing<br>\n",
    " <small>(모델 성능 비교할 수 있도록 임베딩한 document의 내용을 기반으로 한 questions - ground truth 쌍 제작하기)</small>\n",
    "\n",
    "- Make visualization plot for comparing performances of models<br>\n",
    " <small>(모델 성능 비교 결과 시각화 만들기.. PPT 발표용...)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make embeddings using pdf loaders below :\n",
    "\n",
    "# the vesion of PDF Loaders\n",
    "# (1) PyPDFLoader\n",
    "# (2) PyMyPDF4llm\n",
    "# (3) LlamaParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot_test():\n",
    "    def __init__(self, chat_llm):\n",
    "        load_dotenv()\n",
    "\n",
    "        self.llm_version = chat_llm\n",
    "        # the version of OpenAI models\n",
    "        # (1) gpt-3.5-turbo\n",
    "        # (2) gpt-3.5-turbo-0613\n",
    "        # (3) gpt-3.5-turbo-16k-0613\n",
    "        # (4) gpt-3.5-turbo-instruct-0914\n",
    "        # (5) gpt-4\n",
    "        # (6) gpt-4o-mini\n",
    "\n",
    "        OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "        if OPENAI_API_KEY:\n",
    "            print(\"API key loaded successfully.\")\n",
    "        else:\n",
    "            raise ValueError(\"Error loading API key. Check that OPENAI_API_KEY is set inside .env\")\n",
    "\n",
    "        CORPUS_PATH = os.path.join(os.getcwd(), \"corpus\")  \n",
    "        CHROMA_PATH = os.path.join(os.getcwd(), \"data\") \n",
    "\n",
    "        self.chat_history = [] \n",
    "        self.llm = ChatOpenAI(model=self.llm_version, temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "        # => if time left, I'll do experiments on which temperature is ideal between 0 and 1\n",
    "\n",
    "        self.vectorstore = Chroma(persist_directory=CHROMA_PATH, embedding_function=OpenAIEmbeddings())\n",
    "        self.retriever = self.vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"history\", \"context\", \"question\"],\n",
    "            template=\"\"\"\n",
    "            You are a knowledgeable assistant. Use the following pieces of retrieved context to answer the question.\n",
    "            If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "            Conversation history:\n",
    "            {history}\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question:\n",
    "            {question}\n",
    "\n",
    "            Answer:\n",
    "        \"\"\")\n",
    "\n",
    "        print(\"ChatBot initialized successfully!\")    \n",
    "\n",
    "    def format_docs(self, docs):\n",
    "            \"\"\"Format the retrieved documents into a single context string for the prompt.\"\"\"\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        \n",
    "    def format_history(self):\n",
    "        \"\"\"Format the chat history into a string for inclusion in the prompt.\"\"\"\n",
    "        return \"\\n\".join(\n",
    "            f\"Q: {item['question']}\\nA: {item['answer']}\" for item in self.chat_history[-3:]\n",
    "        )\n",
    "\n",
    "    def answer(self, question):\n",
    "        \"\"\"Generate an answer using the RAG chain.\"\"\"\n",
    "        rag_chain = (\n",
    "            {\n",
    "                \"history\": RunnableLambda(lambda _: self.format_history()), \n",
    "                \"context\": self.retriever | self.format_docs,\n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | RunnableLambda(lambda x: x.content) \n",
    "        )\n",
    "\n",
    "        response = rag_chain.invoke(question)\n",
    "        self.chat_history.append({\"question\": question, \"answer\": response})\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Testing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully.\n",
      "ChatBot initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "bot = ChatBot_test(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "questions = [\"What is the topic of this paper?\", \n",
    "             \"What is the main keywords of this paper?\",\n",
    "             \"What is the main sentence of this paper?\",\n",
    "            ]\n",
    "ground_truths = [\"The president said that Justice Breyer has dedicated his life to serve the country and thanked him for his service.\",\n",
    "                \"The president said that Pat Gelsinger is ready to increase Intel's investment to $100 billion.\",\n",
    "                \"The president asked Congress to pass proven measures to reduce gun violence.\"]\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "vectorstore = Chroma(persist_directory=CHROMA_PATH, embedding_function=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "# Inference\n",
    "for query in questions:\n",
    "  answers.append(bot.answer(query))\n",
    "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
    "\n",
    "# To dict\n",
    "data = {\n",
    "    \"user_input\": questions,\n",
    "    \"reference\": ground_truths,\n",
    "    \"response\": answers,\n",
    "    \"retrieved_contexts\": contexts\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "eval_dataset = EvaluationDataset.from_hf_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(features=['user_input', 'retrieved_contexts', 'response', 'reference'], len=3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 6/6 [00:05<00:00,  1.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the topic of this paper?</td>\n",
       "      <td>[]</td>\n",
       "      <td>I'm sorry, but without any specific context or...</td>\n",
       "      <td>The president said that Justice Breyer has ded...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the main keywords of this paper?</td>\n",
       "      <td>[]</td>\n",
       "      <td>I'm sorry, but without any specific context or...</td>\n",
       "      <td>The president said that Pat Gelsinger is ready...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the main sentence of this paper?</td>\n",
       "      <td>[]</td>\n",
       "      <td>I'm sorry, but without any specific context or...</td>\n",
       "      <td>The president asked Congress to pass proven me...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 user_input retrieved_contexts  \\\n",
       "0          What is the topic of this paper?                 []   \n",
       "1  What is the main keywords of this paper?                 []   \n",
       "2  What is the main sentence of this paper?                 []   \n",
       "\n",
       "                                            response  \\\n",
       "0  I'm sorry, but without any specific context or...   \n",
       "1  I'm sorry, but without any specific context or...   \n",
       "2  I'm sorry, but without any specific context or...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  The president said that Justice Breyer has ded...           1.0   \n",
       "1  The president said that Pat Gelsinger is ready...           0.0   \n",
       "2  The president asked Congress to pass proven me...           0.0   \n",
       "\n",
       "   answer_relevancy  \n",
       "0               0.0  \n",
       "1               0.0  \n",
       "2               0.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "result = evaluate(\n",
    "    eval_dataset, metrics = [faithfulness,answer_relevancy], llm = llm_model, embeddings=OpenAIEmbeddings(), raise_exceptions=False\n",
    ")\n",
    "\n",
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5594733088364436, 0.13833670200011863)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2[\"faithfulness\"].mean(), result2[\"faithfulness\"].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.964662193901968, 0.0013356076790130502)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2[\"answer_relevancy\"].mean(), result2[\"answer_relevancy\"].var()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
